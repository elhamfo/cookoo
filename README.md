# Recipe Advisor â€“ Personalized RAG-powered Recipe Chatbot

A modern, full-stack **personalized recipe advisor** chatbot that helps users find or adapt recipes based on ingredients they have, dietary preferences, time constraints, and number of servings.

Built with **FastAPI** (backend) + **React + Tailwind CSS** (frontend) and powered by a **Retrieval-Augmented Generation (RAG)** pipeline using local embeddings and LLM inference.

https://github.com/elhamfo/cookoo

## âœ¨ Features

Semantic search over a recipe dataset using **sentence-transformers** embeddings + **FAISS**
Retrieval-Augmented Generation with **Llama 3.2** (via Ollama) model
Chat interface with message bubbles, loading state, and auto-scroll
Dietary filters & servings input (vegan, gluten-free, quick meals, etc.)
Fully local inference option (no API keys needed when using Ollama)
Clean, responsive UI with Tailwind CSS v4
Swagger/OpenAPI docs for the backend API

## ğŸ—ï¸ Architecture
User â†’ React Frontend (Vite) â†“ FastAPI Backend (localhost:8000) â†“ Query â†’ HuggingFace Embeddings â†’ FAISS Vector Store â†“ Top-k relevant recipes retrieved â†“ Augmented prompt â†’ Llama 3.2 / OpenAI â†’ Generated personalized recipe â†“ Response + sources â†’ displayed in chat

## ğŸš€ Tech Stack

**Backend**
Python 3.11+
FastAPI
LangChain + langchain-ollama / langchain-openai
sentence-transformers (embeddings)
FAISS (vector store)
Ollama (local LLM â€“ Llama 3.2 3B / 1B)

**Frontend**
React 18 (Vite)
Tailwind CSS v4
lucide-react (icons)

**Data**
Public recipe CSV dataset (https://github.com/josephrmartinez/recipe-dataset/blob/main/13k-recipes.csv)

## ğŸ“‹ Quick Start (Local Development)

### Prerequisites

Python 3.11+ 
Node.js 18+
Ollama installed & running (for local LLM)
bash

# 1. Clone repo
git clone https://github.com/elhamfo/cookoo.git
cd recipe-advisor

# 2. Backend setup
python -m venv venv
source venv/bin/activate    # Windows: .\venv\Scripts\activate
pip install -r requirements.txt

# 3. Prepare recipe data & vector index (run once)
python prepare_data.py

# 4. Start backend
python app.py
# â†’ http://localhost:8000/docs (Swagger UI)

# 5. In another terminal â†’ Frontend
cd frontend
npm install
npm run dev
# â†’ http://localhost:5173
